## Scrapers

I wrote two different scripts to scrape store location data - one for each store's website. Caribouscraper.py collects store location data for Caribou Coffee, while starbucksscraper.py collects Starbucks store location info. Both websites structure their store locator pages differently, so I had to customize the code to each one. I chose BeautifulSoup as the Python library to parse the HTML, as I am familiar with it from use on other projects. Both scripts search the store websites for either the ten (Caribou) or fifty (Starbucks) closest stores within a 50 mile radius to every zip code in the Twin Cities MSA, then collects point coordinate, address, and feature data which is archived to a csv file.

Overall, the Starbucks website lent itself to more detailed scraping, as it returned JSON with a large amount of store details. As pandas works more easily with JSON, this made it simple to convert the data into a dataframe. The Caribou website returned less JSON, and for most details besides store coordinates, I had to extract the text from tags.

As for data cleanliness, I have not made a dedicated effort to checking each of the hundreds of points for accuracy. Through the course of visualizing the data, I have picked up some areas where data errors were noticable. Zip codes were one field where errors were common for both Caribou and Starbucks stores. I decided to forego scraping zip code data, and opted to use GIS to find each store's zip code tabulated area (ZCTA). I didn't notice any blatantly erroneous Caribou store coordinates, but some of the Starbucks stores returned coordinates which placed them deep into residential areas. I used Google Maps to find more accurate coordinates for those stores, then rewrote my scraper code to specifically change the coordinates for them. As companies make business decisions, there is also the question of ensuring that stores in the dataset are open and functioning stores. For this reason, I wrote the code so that store data is archived and dated by filename in csv format. The scraper can always be run again when a current set of store data is needed.
